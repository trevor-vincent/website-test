---
title: "Misdiagnosing Cancer by Attacking Machine Learning Models"
date: 2019-09-02T15:34:30-04:00
classes: wide
categories:
  - blog
tags:
  - machine learning
  - genetic algorithm
  - adversarial networks
  - cancer
---

In the kaggle article on [predicting breast cancer using machine learning](https://www.kaggle.com/junkal/breast-cancer-prediction-using-machine-learning), the author uses a SVM model to predict breast cancer in a dataset containing breast mass attributes of patients at a Wisconsin clinic. The author's model is ~99% accurate on test data with an extremely low amount of false negatives (1) and false positives (0). Our goal in this article is to produce a dataset very close to the Wisconsin Kaggle dataset that makes the author's model 100% inaccurate and only produces false positives. In other words, patients would get treated for cancer when they don't have cancer.

To do this we are going to attack the model using a genetic algorithm described in [reference 1 ](#references). What we need is given a feature vector X from the Wisconsin dataset which does not have cancer, e.g.

$$ \rm{MODEL}(X) = \rm{BENIGN} $$

we need to find an attack vector $$ \xi $$ such that

$$ \begin{align*} \rm{MODEL}(X + \xi) &= \rm{MALIGNANT} \\ \rm{where}  \,\,\,\, ||\xi|| &<< ||X|| \end{align*} $$

Genetic algorithms are lots of fun. $$ \xi $$  will be a genetic mutation. If this mutation results in the model assigning a higher probability that the patient has a malignant tumor than other mutations, then we say the population member $$ X + \xi $$ is more fit for breeding. We start off with a population of size N of genetic mutations of $$ X $$. We then breed the fittest population members to make (hopefully) even more fit populations. If this algorithm converges, we will have our adversarial attack vector, $$X_{\rm adv} = X + \xi$$. We then change X to a different feature vector and start again, until we've built a mock dataset filled with benign feature vectors that will be incorrectly labelled malignant.

Let's get started by reproducing the model in the kaggle article. This is very easy:

{% highlight python %}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
       
data = pd.read_csv('breast_cancer_wisconsin_dset.csv', index_col=False)
Y = data['diagnosis'].values
X = data.drop('diagnosis', axis=1).values
X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.20, random_state=20)
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
model = SVC(C=2.0, kernel='rbf')
model.fit(X_train_scaled, Y_train)
X_test_scaled = scaler.transform(X_test)
predictions = model.predict(X_test_scaled)
print("Accuracy score %f" % accuracy_score(Y_test, predictions))
{% endhighlight %}

which scores an accuracy of 99 percent on the test dataset:

{% highlight python %}
Accuracy score 0.991228
{% endhighlight %}

Now we need to write three helper functions. The first function computes the genetic fitness of a population member. There is many
ways one could do this, [reference 1 ](#references) takes the difference of the logs between the target probability and the non-target probabilities. We will
use a simpler method. Since there are only two classifications (cancer and not cancer), the simplest metric for fitness is just the assigned probability for the target. Therefore
we define it as follows:

{% highlight python %}
def compute_fitness(model, x_adv, targetlabel):
    return model.predict_proba([x_adv])[0, targetlabel]
{% endhighlight %}

Secondly, we need a function that defines a mutation 

{% highlight python %}
def mutation(x, delta,  rho):
        noise = np.random.uniform(low = -delta, high = delta, size=x_orig.shape)
        idx_to_apply = bernoulli.rvs(size=x.shape, p = rho)
        x_adv = x + idx_to_apply*noise
        return x_adv
{% endhighlight %}

Lastly, we need a function that takes two fit parents and breeds them. We breed them by randomly sampling features from their
feature vectors based on the relative fitness of each parent. If one parent is more fit, they are sampled more heavily.

{% highlight python %}
def breed(parent1, parent2, fitness1, fitness2):
    total_fitness = fitness1 + fitness2
    sampling_input = [fitness1/(total_fitness), fitness2/(total_fitness)]
    cumsum_si = np.cumsum(sampling_input)
    child = parent2
    for i in range(len(parent1)):
        #"flip a weighted coin" to take gene from a parent
        deviate = np.random.uniform(0.0, 1.0)
        index = np.where(cumsum_si > deviate)[0][0]
        if index == 0:
            child[i] = parent1[i]
    return child
{% endhighlight %}

Putting it all together,

{% highlight python %}
#Function to sample from a probability distribution
def sampling_dist(prob_dist):
    prob_dist_cumsum = np.cumsum(prob_dist)
    deviate = np.random.uniform(0.0, 1.0)
    index = np.where(prob_dist_cumsum > deviate)[0][0]
    return index

def genetic_attack(model, x, delta, rho, pop_size, num_of_generations, targetlabel):
    score_history = []
    population = [[0 for i in range(pop_size)] for g in range(num_of_generations)]
    fitnesses = np.zeros((num_of_generations, pop_size))
    
    #create initial population
    for i in range(0, pop_size):
        population[0][i]  = mutation(x, delta, rho) 

    #start evolution
    for g in range(1, num_of_generations):
        
        #compute fitness of last generations population
        for i in range(0, pop_size):
            fitnesses[g-1, i] = compute_fitness(model, population[g-1][i], targetlabel)
        
        #find fittest member in last generation
        adv_pop_idx = np.argmax(fitnesses[g-1])
        score_history.append(fitnesses[g-1][adv_pop_idx])
        x_adv = population[g-1][adv_pop_idx]
        #if fittest member is fit enough, stop algorithm, otherwise continue
        if g % 1000 == 0:
            print('cur_gen:%d, cur_score:%.5f' %(g, fitnesses[g-1][adv_pop_idx]))
        if (model.predict_proba([x_adv])[0,targetlabel] > .5):
            return score_history
        
        #add fittest member to next generation
        population[g][0] = x_adv       
        
        #breed children from fit parents for next generation
        prob_dist = fitnesses[g-1, :]/(fitnesses[g-1, :].sum())
        
        for i in range(1, pop_size):
            parent_1_idx = sampling_dist(prob_dist)
            parent_2_idx = sampling_dist(prob_dist)
            parent_1 = population[g-1][parent_1_idx]
            parent_2 = population[g-1][parent_2_idx]            
            child = breed(parent_1, parent_2, fitnesses[g-1, parent_1_idx], fitnesses[g-1, parent_2_idx])
            #mutate the child before adding to next generation
            population[g][i] = mutation(child, delta, rho)
            
    return score_history   
{% endhighlight %}

Running the code:

{% highlight python %}
score_history = genetic_attack(model, X_train_scaled[0], .01, .1, 100, 10000, 1)
{% endhighlight %}

![](https://trevor-vincent.github.io/website/images/posts/genattack.png)

With a population size of a hundred and 8000 generations, we arrive at a $$X_{\rm adv}$$ feature vector with fitness greater than .5 (i.e. the model would predict that it is malignant), but which differs from a feature vector $$ X $$, which is benign, by no more than .01 in any feature. We didn't have to stop at .01, the algorithm would most likely converge for any small number, it would just take a while.

Get the full code [here](https://github.com/trevor-vincent/python_examples/blob/master/MachineLearning/GenAttack/genattack_cancer.ipynb).

# References

1. https://arxiv.org/abs/1805.11090
